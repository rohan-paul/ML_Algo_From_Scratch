{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## Compute performance metrics for the given data 5_a.csv\n",
    "\n",
    "  Note 1: in this data you can see number of positive points >> number of negatives points\n",
    "\n",
    "  Note 3: you need to derive the class labels from given score\n",
    "\n",
    "  $y^{pred}= \\text{[0 if y_score < 0.5 else 1]}$\n",
    "\n",
    "\n",
    " - Compute Confusion Matrix\n",
    "\n",
    " - Compute F1 Score\n",
    "\n",
    " - Compute AUC Score, you need to compute different thresholds and for each threshold compute tpr,fpr and then use\n",
    "\n",
    " numpy.trapz(tpr_array, fpr_array)\n",
    "\n",
    " https://stackoverflow.com/q/53603376/4084039\n",
    "\n",
    " https://stackoverflow.com/a/39678975/4084039\n",
    "\n",
    " Note: it should be numpy.trapz(tpr_array, fpr_array) not numpy.trapz(fpr_array, tpr_array)\n",
    "\n",
    "- Compute Accuracy Score"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# df_5_a = pd.read_csv('../Instructions-AML/5_a.csv')\n",
    "df_5_a = pd.read_csv('https://raw.githubusercontent.com/rohan-paul/Multiple-Dataset/main/5-Performance-metrics-without-Sklearn/5_a.csv')\n",
    "print(df_5_a.shape)\n",
    "df_5_a.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_5_a['y_predicted'] = np.where(df_5_a['proba'] >= 0.5, float(1), float(0))\n",
    "df_5_a.head()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Checking to see if there's any 'proba' less than or equal to 0.5\n",
    "# And there is none. So all y_predicted will be classified as 1\n",
    "# df = df_5_a.loc[df_5_a['proba'] <= 0.5 ]\n",
    "# df.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# print(df_5_a.to_numpy())\n",
    "actual_y_train_arr = df_5_a.iloc[:, 0].values\n",
    "print('actual_y_train_arr ', actual_y_train_arr)\n",
    "predicted_y_arr = df_5_a.iloc[:, 2].values\n",
    "print('predicted_y_arr ', predicted_y_arr)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### In below implementation, for a binary class-label (1 and 0 ) I will have 'true-nagative' at the top left of the final confusion matrix, as the traversing of the unique_classes array will start from 0\n",
    "\n",
    "![img](https://i.imgur.com/MvapmPv.png[/img])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "def calculate_confusion_matrix(true_y_classes_array, predicted_y_classes_array):\n",
    "\n",
    "  # extract all unique classes from the train y class\n",
    "  unique_classes = np.unique(true_y_classes_array)\n",
    "  # print('unique', unique_classes)\n",
    "\n",
    "  # initialize a matrix with zero values that will be the final confusion matrix\n",
    "  confusion_matrix = np.zeros((len(unique_classes), len(unique_classes)))\n",
    "\n",
    "  for i in range(len(unique_classes)):\n",
    "    for j in range(len(unique_classes)):\n",
    "      confusion_matrix[i, j] = np.sum((true_y_classes_array == unique_classes[i]) & (predicted_y_classes_array == unique_classes[j]))\n",
    "\n",
    "  return confusion_matrix\n",
    "\n",
    "# actual_y_class_list = [1, 3, 3, 2, 5, 5, 3, 2, 1, 4, 3, 2, 1, 1, 2]\n",
    "# predicted_y_class_list = [1, 2, 3, 4, 2, 3, 3, 2, 1, 2, 3, 1, 5, 1, 1]\n",
    "\n",
    "# Only binary class dataset\n",
    "actual_y_class_list = [1, 1, 1, 0, 0]\n",
    "predicted_y_class_list =   [1, 1, 1, 0, 1]\n",
    "\n",
    "print(calculate_confusion_matrix(actual_y_class_list, predicted_y_class_list))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Explanations on the above calculate_confusion_matrix() funcion\n",
    "\n",
    "[In above implementation, for a binary class-label (1 and 0 ) I will have 'true-nagative' at the top left of the final confusion matrix, as the traversing of the unique_classes array will start from 0 ]\n",
    "\n",
    "#### 1.np.zeros()  - the first arg of np.zeros() is the shape which is a tuple of ints or simple int\n",
    "\n",
    "e.g., (2, 3) or 2.\n",
    "\n",
    "---\n",
    "\n",
    "#### 2. Now for each row, I need to compare the values between true_y_classes_array and predicted_y_classes_array\n",
    "\n",
    "So, I will implement this by comparing each element from true_y_classes_array with each of the unique array's elements and then the same for  predicted_y_classes_array\n",
    "\n",
    "It will give me a list of True / False. Lets see this example\n",
    "\n",
    "```python\n",
    "arr1 = [1, 1, 2, 6, 4]\n",
    "arr2 = np.unique(arr1) # [1 2 4 6]\n",
    "\n",
    "print(\"comparing \",  arr1 == arr2[0])\n",
    "# comparing [ True  True False False False]\n",
    "```\n",
    "\n",
    "That is, the above will compare the whole of arr1 with arr2[0] which is 1\n",
    "\n",
    "---\n",
    "\n",
    "#### 3. An now use np.sum() to count number of True value\n",
    "\n",
    "```python\n",
    "print('np.sum of above True False numpy array ', np.sum(arr1 == arr2[0]))\n",
    "\n",
    "# np.sum of above True False numpy array  2\n",
    "```\n",
    "\n",
    "Note that above kind of operation will only work with `np.unique()` which returns a numpy array, and will not work with plain python list\n",
    "\n",
    "---\n",
    "\n",
    "### 4. So now for the confusion matrix, I need to fill up with matching counts of\n",
    "\n",
    "#### Note `unique_classes[0]` is 1 and `unique_classes[1]` = 0\n",
    "\n",
    "### For first row of my final confusion_matrix\n",
    "\n",
    "confusion_matrix[0,0] => i.e. i, j = 0, 0 => will have the Total 'True' count (i.e. `np.sum()`)  of following conditions\n",
    "\n",
    "`(true_y_classes_array == unique_classes[0]) & (predicted_y_classes_array == unique_classes[0])`\n",
    "\n",
    "Similarly for `confusion_matrix[0, 1]` => i.e. i, j = 0, 1  => will have the Total 'True' count (i.e. `np.sum()`)  of following conditions\n",
    "\n",
    "`(true_y_classes_array == unique_classes[0]) & (predicted_y_classes_array == unique_classes[1])`\n",
    "\n",
    "#### And for second row of my final confusion_matrix\n",
    "\n",
    "confusion_matrix[1,0] => will have the Total 'True' count (i.e. `np.sum()`)  of following conditions\n",
    "\n",
    "`(true_y_classes_array == unique_classes[1]) & (predicted_y_classes_array == unique_classes[0])`\n",
    "\n",
    "Similarly for confusion_matrix[1, 1] => will have the Total 'True' count (i.e. `np.sum()`)  of following conditions\n",
    "\n",
    "`(true_y_classes_array == unique_classes[1]) & (predicted_y_classes_array == unique_classes[1])`\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "---\n",
    "\n",
    "### If for a binary class-label I have to place 'true-positive' / TF at the top left of the final confusion matrix\n",
    "\n",
    "i.e. the top-left will have 1 instead of 0 in the final confusion matrix then only need to reverse the unique_classes variable as below."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3. 1.]\n",
      " [0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "def confusion_matrix_tf_top_left(true_y_classes_array, predicted_y_classes_array):\n",
    "\n",
    "  # extract all unique classes from the train y class\n",
    "  unique_classes = np.unique(true_y_classes_array)\n",
    "  # For a binary class the above will give me [0 1] numpy array\n",
    "  # so top-left of confusion matrix will start from 0 i.e. 'True Negative'\n",
    "\n",
    "  # But the challenge here asks that the top left will be 'True Positive'\n",
    "  # Hence I need to reverse the above numpy array\n",
    "  unique_classes = unique_classes[::-1]\n",
    "  # print('reversed unique', unique_classes) # will convert the above array to [1 0]\n",
    "\n",
    "  # initialize a matrix with zero values that will be the final confusion matrix\n",
    "  confusion_matrix = np.zeros((len(unique_classes), len(unique_classes)))\n",
    "\n",
    "  for i in range(len(unique_classes)):\n",
    "    for j in range(len(unique_classes)):\n",
    "          # replace below line\n",
    "          # confusion_matrix[i, j] = np.sum((true_y_classes_array == unique_classes[i]) & (predicted_y_classes_array == unique_classes[j]))\n",
    "          # with below\n",
    "          confusion_matrix[i, j] = np.sum((true_y_classes_array == unique_classes[j]) & (predicted_y_classes_array == unique_classes[i]))\n",
    "\n",
    "  return confusion_matrix\n",
    "\n",
    "# actual_y_class_list = [1, 3, 3, 2, 5, 5, 3, 2, 1, 4, 3, 2, 1, 1, 2]\n",
    "# predicted_y_class_list = [1, 2, 3, 4, 2, 3, 3, 2, 1, 2, 3, 1, 5, 1, 1]\n",
    "\n",
    "actual_y_class_list = [1, 1, 1, 0, 0]\n",
    "predicted_y_class_list =   [1, 1, 1, 0, 1]\n",
    "\n",
    "confusion_matrix_tf_top_left = confusion_matrix_tf_top_left(actual_y_class_list, predicted_y_class_list)\n",
    "print(confusion_matrix_tf_top_left)\n",
    "\n",
    "true_negative, false_positive, false_negative, true_positive = confusion_matrix_tf_top_left[1][1], confusion_matrix_tf_top_left[0][1], confusion_matrix_tf_top_left[1][0], confusion_matrix_tf_top_left[0][0]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Explanations and notes on above Confusion matrix function\n",
    "\n",
    "![img](https://i.imgur.com/1A3Izpg.png)\n",
    "\n",
    "#### Note `unique_classes[0]` is 1 and `unique_classes[1]` = 0\n",
    "\n",
    "### For first row of my final confusion_matrix\n",
    "\n",
    "`confusion_matrix[0,0]` => i.e. i, j = 0, 0 => will have the Total 'True' count (i.e. `np.sum()`) of following conditions\n",
    "\n",
    "`(true_y_classes_array == unique_classes[0]) & (predicted_y_classes_array == unique_classes[0])`\n",
    "\n",
    "Similarly for `confusion_matrix[0, 1]` => i.e. i, j = 0, 1 => will have the Total 'True' count (i.e. `np.sum()`)  of following conditions\n",
    "\n",
    "#### Change this line from previous implementation (when \"True Negative\" was at top-left of Confusion Matrix ) -\n",
    " `(true_y_classes_array == unique_classes[0]) & (predicted_y_classes_array == unique_classes[1])`\n",
    "\n",
    "TO\n",
    "\n",
    "`(true_y_classes_array == unique_classes[1]) & (predicted_y_classes_array == unique_classes[0])`\n",
    "\n",
    "---\n",
    "\n",
    "### Now second row\n",
    "\n",
    "And for second row of my final confusion_matrix\n",
    "\n",
    "`confusion_matrix[1,0]`  => i.e. i, j = 1, 0 => will have the Total 'True' count (i.e. `np.sum()`)  of following conditions\n",
    "\n",
    "#### Change this line from previous implementation (when \"True Negative\" was at top-left of Confusion Matrix ) -\n",
    "`(true_y_classes_array == unique_classes[1]) & (predicted_y_classes_array == unique_classes[0])`\n",
    "\n",
    "TO\n",
    "\n",
    "`(true_y_classes_array == unique_classes[0]) & (predicted_y_classes_array == unique_classes[1])`\n",
    "\n",
    "\n",
    "\n",
    "Similarly for `confusion_matrix[1, 1]`  => i.e. i, j = 1, 1  => will have the Total 'True' count (i.e. `np.sum()`) of following conditions\n",
    "\n",
    "\n",
    "`(true_y_classes_array == unique_classes[1]) & (predicted_y_classes_array == unique_classes[1])`"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "# sum-all-the-elements-of-the confusion_matrix_tf_top_left\n",
    "# To check that the total num of elements of the original dataframe matches\n",
    "# with the counts captured in the confusion matrix\n",
    "sum_all_elements_of_confusion_matrix = np.concatenate(confusion_matrix_tf_top_left).sum()\n",
    "print(sum_all_elements_of_confusion_matrix == len(actual_y_class_list))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 1]\n",
      " [0 3]]\n"
     ]
    }
   ],
   "source": [
    "# Testing my custom confusion_matrix result with scikit-learn\n",
    "from sklearn.metrics import confusion_matrix\n",
    "sklearn_confustion_matrix = confusion_matrix(actual_y_class_list, predicted_y_class_list)\n",
    "print(sklearn_confustion_matrix)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 1 0 3\n",
      "1.0 1.0 0.0 3.0\n"
     ]
    }
   ],
   "source": [
    "tn, fp, fn, tp = confusion_matrix(actual_y_class_list, predicted_y_class_list).ravel()\n",
    "print(tn, fp, fn, tp)\n",
    "print(true_negative, false_positive, false_negative, true_positive)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### From above we can see the values of the confution Matrix matches between scikit-learn and our custom-implementation\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "## F1 Score\n",
    "\n",
    "![img](https://i.imgur.com/ZPntYB0.jpg)\n",
    "\n",
    "![Imgur](https://imgur.com/qy5Fesd.jpg)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# the below function will work only for\n",
    "# binary confusion matrix\n",
    "def get_f1_accuracy(binary_conf_matrix):\n",
    "    true_negative  = binary_conf_matrix[1][1]\n",
    "    false_positive = binary_conf_matrix[0][1]\n",
    "    false_negative = binary_conf_matrix[1][0]\n",
    "    true_positive = binary_conf_matrix[0][0]\n",
    "\n",
    "    precision = true_positive / (true_positive + false_positive)\n",
    "    recall = true_positive/ (true_positive + false_negative)\n",
    "\n",
    "    f1_score = (2 * (precision * recall)) / (precision + recall )\n",
    "\n",
    "    sum_all_elements_of_confusion_matrix = np.concatenate(binary_conf_matrix).sum()\n",
    "\n",
    "    accuracy_score = (true_positive + true_negative)/sum_all_elements_of_confusion_matrix\n",
    "\n",
    "    return f1_score, accuracy_score\n",
    "\n",
    "\n",
    "print(get_f1_accuracy(confusion_matrix_tf_top_left))\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Now verifying the above F1-Score with that of sk-learn\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "sklearn_f1_score = f1_score(actual_y_train_arr, predicted_y_arr)\n",
    "print(sklearn_f1_score)\n",
    "\n",
    "sklearn_accuracy_score = accuracy_score(actual_y_train_arr, predicted_y_arr)\n",
    "print(sklearn_accuracy_score)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### From above we can see the values of1_score, accuracy_score matches between scikit-learn and our custom-implementation\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "## AUC-ROC Score Calculation\n",
    "\n",
    "The Receiver Operating Characetristic (ROC) curve is a graphical plot that allows us to assess the performance of binary classifiers. With imbalanced datasets, the Area Under the Curve (AUC) score is calculated from ROC and is a very useful metric in imbalanced datasets.\n",
    "\n",
    "TPR and FPR are defined as follows:\n",
    "\n",
    "- TPR = True Positives / All Positives\n",
    "- FPR = False Positives / All negatives"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def get_tpr_fpr(y_actual, y_probabilities, threshold_proba_array):\n",
    "    tp = 0\n",
    "    fp = 0\n",
    "    tn = 0\n",
    "    fn = 0\n",
    "    for i in range(len(y_probabilities)):\n",
    "        if y_probabilities[i] >= threshold_proba_array :\n",
    "            if y_actual[i] == 1:\n",
    "                tp += 1\n",
    "            else:\n",
    "                fp += 1\n",
    "\n",
    "        if y_probabilities[i] < threshold_proba_array:\n",
    "            if y_actual[i] == 0:\n",
    "                tn += 1\n",
    "            else:\n",
    "                fn += 1\n",
    "\n",
    "    tpr = tp / (tp + fn )\n",
    "    fpr = fp / (fp + tn)\n",
    "\n",
    "    return [tpr, fpr]\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "unique_probability_thresholds = np.unique(df_5_a['proba'])\n",
    "print(unique_probability_thresholds)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "y_proba = df_5_a.iloc[:, 1].values\n",
    "# print(actual_y_train_arr)\n",
    "# print(y_proba)\n",
    "# And we alredy have the earlier calculted 'actual_y_train_arr'\n",
    "\n",
    "roc_array = []\n",
    "# The above roc_array will be a 2-D array where each inner elements will be an\n",
    "# array of [tpr, fpr]. Hence the full roc_array will be of the form\n",
    "# [[tpr1, fpr1], [tpr2, fpr2], ....[tprn, fprn] ]\n",
    "\n",
    "revesed_unique_probability_thresholds =unique_probability_thresholds[::-1]\n",
    "# Need to reverse the above array for use in np.trapz(),\n",
    "# else will get negative roc_auc_score\n",
    "# https://stackoverflow.com/a/11974971/1902852\n",
    "# In trapz(x,y) differentiation of x is applied through diff(x,1,1), i.e. [x(2:n,:) - x(1:n-1,:)]. If your x is descending this will give negative dx. However, it doesn't matter if it is positive or negative.\n",
    "for threshold in revesed_unique_probability_thresholds:\n",
    "    tpr_fpr_arr = get_tpr_fpr(actual_y_train_arr, y_proba, threshold)\n",
    "    roc_array.append(tpr_fpr_arr)\n",
    "\n",
    "# print(roc_array)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Now I need to get all tpr_array together and also all fpr_array together\n",
    "\n",
    "all_tpr_together = []\n",
    "all_fpr_together = []\n",
    "for i in range(len(roc_array) - 1):\n",
    "    point_a = roc_array[i]\n",
    "    point_b = roc_array[i + 1]\n",
    "    all_tpr_together.append([point_a[0], point_b[0]])\n",
    "    all_fpr_together.append([point_a[1], point_b[1]])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## A note on numpy.trapz() function\n",
    "\n",
    "#### numpy.trapz() function integrate along the given axis using the composite [trapezoidal rule](https://en.wikipedia.org/wiki/Trapezoidal_rule).\n",
    "\n",
    "![Imgur](https://imgur.com/kKX9G5R.png)\n",
    "\n",
    "![Imgur](https://imgur.com/6zwXG8w.png)\n",
    "\n",
    "You specify the integration range when you pass the x array to the np.trapz function.\n",
    "\n",
    "\n",
    "#### When can I get a negative value from numpy.trapz()\n",
    "\n",
    "#### If the $x_i$ in the call to trapz() are not in increasing order, then you will get negative results. So if you want a positive number from this function when it returns a negative, Try re-ordering x in ascending order (and y-values accordingly):\n",
    "\n",
    "```python\n",
    "x_order = x(end:-1:1); %fliplr\n",
    "y_order = y(end:-1:1); %fliplr\n",
    "trapz(x_order, y_order)\n",
    "```\n",
    "\n",
    " In essence, I will get negative result, when I am integrating the function represented by the y data from x = 1 to x = 0, not from x = 0 to x = 1. If I flip my x vector so that I am integrating from x = 0 to x = 1 (essentially swapping the limits of integration) then the area will be positive.\n",
    "\n",
    "In `trapz(x,y)` differentiation of x is applied through diff(x,1,1), i.e. `[x(2:n,:) - x(1:n-1,:)].` If your x is descending this will give negative dx. It doesn't matter if it is positive or negative. However, in `plot` the curve will appear positive-definite (you don't actually see the order of points, just pairs from two vectors on a plane).\n",
    "\n",
    "**Example** (compare the following):\n",
    "\n",
    "```python\n",
    "x = [-1 -0.5 0]; y = 0.5-x;\n",
    "figure; plot(x,y); hold on; plot(-x, y,'r')\n",
    "trapz(x, y)\n",
    "trapz(-x, y)\n",
    "figure; plot(x, y); hold on; plot(fliplr(-x), fliplr(y),'r')\n",
    "trapz(fliplr(-x), fliplr(y))\n",
    "```\n",
    "\n",
    "\n",
    "Think of it like this. The integral of a function that is always positive, if the limits are inverted, will still be negative. Thus we know that\n",
    "\n",
    "    int(x^2,-1,1) = 2/3\n",
    "\n",
    "But\n",
    "\n",
    "    int(x^2,1,-1) = -2/3\n",
    "\n",
    "Clearly x^2 is always a positive number, but here the limits of integration were not increasing, but decreasing.\n",
    "\n",
    "If the $x_i$ in the call to trapz are not in increasing order, then you will get negative results. This is reflected by trapz. Trapz sees the order of the points as presented to it.\n",
    "\n",
    "    x = -1:.1:1;\n",
    "    trapz(x,x.^2)\n",
    "    ans =\n",
    "             0.67\n",
    "\n",
    "    xrev = fliplr(x);\n",
    "    trapz(xrev,xrev.^2)\n",
    "    ans =\n",
    "            -0.67\n",
    "\n",
    "The plot shows only that the function is positive, not the order of the points.\n",
    "\n",
    "---\n",
    "\n",
    "### Now finally the AUC Score\n",
    "(Note it may take few minutes to run, as it will cycle through all the threshold )"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "auc_score = sum(np.trapz(all_tpr_together, all_fpr_together))\n",
    "print('My Custom function ROC-AUC Score for 5_a.csv: ', auc_score)\n",
    "# My Custom function ROC-AUC Score for 5_a.csv:  0.48829899999999987"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Checking result with scikit-learn\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "actual_y_train_arr = df_5_a.iloc[:, 0].values\n",
    "\n",
    "sklearn_roc_auc_score = roc_auc_score(actual_y_train_arr, y_proba)\n",
    "print('sk-learn roc_auc_score for 5_a.csv: ', sklearn_roc_auc_score)\n",
    "# sk-learn roc_auc_score:  0.48829900000000004"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### As we can see above the roc_auc_score mathches to a high decimal points between my Custom function and that of sklearn"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Checking result with scikit-learn\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "actual_y_train_arr = df_5_a.iloc[:, 0].values\n",
    "\n",
    "sklearn_roc_auc_score = roc_auc_score(actual_y_train_arr, y_proba)\n",
    "print('sk-learn roc_auc_score - ', sklearn_roc_auc_score)\n",
    "# sk-learn roc_auc_score -  0.48829900000000004"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Annotating threshold value in the RUC-AOC curve\n",
    "\n",
    "When you only plot the TPR and the FPR against each other you'll loose the threshold information. However, you can easily add them to the plot. In the below example with some randomly generated X and Y data, I annotated every 5th value but this should be enough the see the relationship (high confidence - bottom left, low confidence - top right)."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "x = np.random.randint(40, 400, 100).reshape(-1, 1)\n",
    "y = np.random.randint(0, 2, 100)\n",
    "\n",
    "model = LogisticRegression()\n",
    "model.fit(x, y)\n",
    "probs = model.predict_proba(x)\n",
    "fpr, tpr, thresholds = metrics.roc_curve(y, probs[:,1])\n",
    "\n",
    "# %%\n",
    "plt.subplots(figsize=(10, 6))\n",
    "plt.plot(fpr, tpr, 'o-', label=\"ROC curve\")\n",
    "plt.plot(np.linspace(0,1,10), np.linspace(0,1,10), label=\"diagonal\")\n",
    "for x, y, txt in zip(fpr[::5], tpr[::5], thresholds[::5]):\n",
    "    plt.annotate(np.round(txt,2), (x, y-0.04))\n",
    "rnd_idx = 27\n",
    "plt.annotate('this point refers to the tpr and the fpr\\n at a probability threshold of {}'.format(np.round(thresholds[rnd_idx], 2)),\n",
    "             xy=(fpr[rnd_idx], tpr[rnd_idx]), xytext=(fpr[rnd_idx]+0.2, tpr[rnd_idx]-0.25),\n",
    "             arrowprops=dict(facecolor='black', lw=2, arrowstyle='->'),)\n",
    "plt.legend(loc=\"upper left\")\n",
    "plt.xlabel(\"FPR\")\n",
    "plt.ylabel(\"TPR\")\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## After plotting ROC-Curve - how the threshold relates back to the values of the variable (x) for identification of the cut off.\n",
    "\n",
    "Simple ans is we can not.\n",
    "\n",
    "X was our input matrix on which we performed the prediction. The thresholds are only related to the prediction from the classifier (\"probabilities\" values)."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "## Next Task - Computing the best threshold\n",
    "\n",
    "Compute the best threshold (similarly to ROC curve computation) of probability which gives lowest values of metric A for the given data 5_c.csv\n",
    "\n",
    "you will be predicting label of a data points like this:\n",
    "\n",
    "$y^{pred}= \\text{[0 if y_score < threshold  else 1]}$\n",
    "\n",
    "$ A = 500 \\times \\text{number of false negative} + 100 \\times \\text{numebr of false positive}$\n",
    "\n",
    "Note 1: in this data you can see number of negative points > number of positive points"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# df_5_c = pd.read_csv('../Instructions-AML/5_c.csv')\n",
    "df_5_c = pd.read_csv('https://raw.githubusercontent.com/rohan-paul/Multiple-Dataset/main/5-Performance-metrics-without-Sklearn/5_c.csv')\n",
    "print('df_5_c.shape ', df_5_c.shape)\n",
    "df_5_c.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "actual_y_train_arr_5_c = df_5_c.iloc[:, 0].values\n",
    "print('actual_y_train_arr_5_c ', actual_y_train_arr_5_c)\n",
    "\n",
    "y_proba_5_c = df_5_c.iloc[:, 1].values\n",
    "print('y_proba_5_c ', y_proba_5_c)\n",
    "\n",
    "unique_probability_thresholds_5_c = np.unique(df_5_c['prob'])\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# First I will modify the above get_tpr_fpr() function to return the value of A\n",
    "\n",
    "def get_A_metric(y_actual, y_probabilities, threshold):\n",
    "    tp = 0\n",
    "    fp = 0\n",
    "    tn = 0\n",
    "    fn = 0\n",
    "\n",
    "    min_a = float('inf')\n",
    "\n",
    "    for i in range(len(y_probabilities)):\n",
    "        if y_probabilities[i] >= threshold :\n",
    "            if y_actual[i] == 1:\n",
    "                tp += 1\n",
    "            else:\n",
    "                fp += 1\n",
    "\n",
    "        if y_probabilities[i] < threshold:\n",
    "            if y_actual[i] == 0:\n",
    "                tn += 1\n",
    "            else:\n",
    "                fn += 1\n",
    "\n",
    "    A = (500 * fn) + (100 * fp)\n",
    "\n",
    "    return A\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Now a function to traverse the entire unique probability thresholds array\n",
    "# and return the minimum value of A and also the corresponding threshold\n",
    "\n",
    "def get_minimized_a(y_actual, y_probabilities, total_threshold_arr ):\n",
    "  min_a = float('inf')\n",
    "  min_t = 0\n",
    "\n",
    "  for threshold in total_threshold_arr:\n",
    "    a = get_A_metric(y_actual, y_probabilities, threshold)\n",
    "    if a <= min_a:\n",
    "        min_a = min(a, min_a)\n",
    "        min_t = threshold\n",
    "\n",
    "  return min_a, min_t"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(get_minimized_a(actual_y_train_arr_5_c, y_proba_5_c, unique_probability_thresholds_5_c ))\n",
    "# (141000, 0.2300390278970873)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "So that thats the minimum value of **A** which is 141000\n",
    "\n",
    "and the corresponding threshold is 0.2300390278970873\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "## D. Compute performance metrics(for regression) for the given data 5_d.csv\n",
    "\n",
    "Note 1: 5_d.csv will having two columns Y and predicted_Y both are real valued features\n",
    "\n",
    "Compute Mean Square Error\n",
    "\n",
    "Compute MAPE:\n",
    "\n",
    "Compute R^2 error: https://en.wikipedia.org/wiki/Coefficient_of_determination#Definitions\n",
    "\n",
    "---"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "\n",
    "# df_5_d = pd.read_csv('../Instructions-AML/5_d.csv')\n",
    "df_5_d = pd.read_csv('https://raw.githubusercontent.com/rohan-paul/Multiple-Dataset/main/5-Performance-metrics-without-Sklearn/5_d.csv')\n",
    "df_5_d.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# print(df_5_d.to_numpy())\n",
    "actual_y_train_arr_5d = df_5_d.iloc[:, 0].values\n",
    "print('actual_y_train_arr_5d ', actual_y_train_arr_5d)\n",
    "predicted_y_arr_5d = df_5_d.iloc[:, 1].values\n",
    "print('predicted_y_arr_5d ', predicted_y_arr_5d)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Coefficient of determination also called as R2 score is used to evaluate the performance of a linear regression model. It is the amount of the variation in the output dependent attribute which is predictable from the input independent variable(s). It is used to check how well-observed results are reproduced by the model, depending on the ratio of total deviation of results described by the model.\n",
    "\n",
    "Assume R2 = 0.68\n",
    "It can be referred that 68% of the changeability of the dependent output attribute can be explained by the model while the remaining 32 % of the variability is still unaccounted for.\n",
    "R2 indicates the proportion of data points which lie within the line created by the regression equation. A higher value of R2 is desirable as it indicates better results.\n",
    "\n",
    "R² is calculated by taking one minus the sum of squares of residuals divided by the total sum of squares.\n",
    "\n",
    "![Imgur](https://imgur.com/X0zWQho.png)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Checking the r-square metric with sklearn\n",
    "sklearn_r2 = r2_score(actual_y_train_arr_5d, predicted_y_arr_5d)\n",
    "sklearn_r2"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def calculate_r2_score(y_train, y_predicted):\n",
    "    y_train_bar = y_train.mean()\n",
    "    # y_train_bar = np.mean(y_train)\n",
    "\n",
    "    sum_squared_residual = ((y_train - y_predicted)**2).sum()\n",
    "    sum_squared_total = ((y_train - y_train_bar)**2).sum()\n",
    "\n",
    "    return 1 - (sum_squared_residual/sum_squared_total)\n",
    "\n",
    "print(calculate_r2_score(actual_y_train_arr_5d, predicted_y_arr_5d))\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "So the r-square measures matches between my custom function and sklearn's inbuilt funcion\n",
    "\n",
    "---\n",
    "\n",
    "## Mean Squared Error (MSE) / Mean Squared Deviation (MSD)\n",
    "\n",
    "The Mean Squared Error measures the average of the errors squared. It basically calculates the difference between the estimated and the actual value, squares these results and then computes their average.\n",
    "\n",
    "Because the errors are squared, MSE can only assume non-negative values. Due to the intrinsic randomness and noise associated with most processes, MSE is usually positive and not zero.\n",
    "\n",
    "![Imgur](https://imgur.com/GWH0ap5.png)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def calculate_mse(y_actual, y_predicted):\n",
    "    mse = np.mean((y_actual - y_predicted)**2)\n",
    "    return mse\n",
    "\n",
    "print(calculate_mse(actual_y_train_arr_5d, predicted_y_arr_5d))\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Checking the Mean Square Error metric with sklearn\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "sklearn_mse = mean_squared_error(actual_y_train_arr_5d, predicted_y_arr_5d)\n",
    "sklearn_mse"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "So the Mean Squared Error measures matches between my custom function and sklearn's inbuilt funcion\n",
    "\n",
    "---\n",
    "\n",
    "## Mean Absolute Percentage Error (MAPE)\n",
    "\n",
    "The Mean Absolute Percentage Error measures the error between actual and forecasted values as a percentage. It achieves so by calculating it similarly to MAE, but also dividing it by the actual value, expressing the result as a percentage.\n",
    "\n",
    "By expressing the error as a percentage, we can have a better understanding of how off our predictions are in relative terms. For instance, if we were to predict next year’s spending, an MAE error of $50 could be both a relatively good or bad approximation.\n",
    "\n",
    "![Imgur](https://imgur.com/zCFJTE6.png)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# The below will return the % value i.e 12.9 means 12.9%\n",
    "def calculate_mean_absolute_percentage_error(y_actual, y_predicted):\n",
    "    mape = np.mean((np.abs(y_actual - y_predicted)) / np.mean(y_actual)) * 100\n",
    "    return mape\n",
    "\n",
    "print(calculate_mean_absolute_percentage_error(actual_y_train_arr_5d, predicted_y_arr_5d))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### When one of the actual data-point is zero\n",
    "\n",
    "https://en.wikipedia.org/wiki/Mean_absolute_percentage_error\n",
    "\n",
    "Problems can occur when calculating the MAPE value with a series of small denominators. A singularity problem of the form 'one divided by zero' and/or the creation of very large changes in the Absolute Percentage Error, caused by a small deviation in error, can occur.\n",
    "\n",
    "As an alternative, each actual value (At) of the series in the original formula can be replaced by the average of all actual values (Āt) of that series. This alternative is still being used for measuring the performance of models that forecast spot electricity prices.[2]\n",
    "\n",
    "Note that this is equivalent to dividing the sum of absolute differences by the sum of actual values, and is sometimes referred to as WAPE (weighted absolute percentage error).\n",
    "\n",
    "So in that case the formulate becomes\n",
    "\n",
    "![Imgur](https://imgur.com/KTg47Gk.png)\n",
    "\n",
    "The derivation is as follows.\n",
    "\n",
    "![Imgur](https://imgur.com/Lm45BRD.png)\n",
    "\n",
    "Basically,\n",
    "\n",
    "```python\n",
    "\n",
    "mean(actual_value) = sum(actual_value) / n\n",
    "\n",
    "hence n * mean(actual_value = sum(a)\n",
    "\n",
    "```\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def calculate_mean_absolute_percentage_error_2(y_actual, y_predicted):\n",
    "    mape = ((np.sum(np.abs(y_actual - y_predicted))) / np.sum(y_actual)) * 100\n",
    "    return mape\n",
    "\n",
    "print(calculate_mean_absolute_percentage_error_2(actual_y_train_arr_5d, predicted_y_arr_5d))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}