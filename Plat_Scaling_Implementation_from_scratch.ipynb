{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": 2,
            "source": [
                "import numpy as np\n",
                "import pandas as pd\n",
                "from sklearn.datasets import make_classification\n",
                "import numpy as np\n",
                "from sklearn.svm import SVC\n",
                "from sklearn.svm import LinearSVC\n",
                "from sklearn.model_selection import train_test_split\n",
                "import math\n",
                "import matplotlib.pyplot as plt\n",
                "\n",
                "X, y = make_classification(n_samples=5000, n_features=5, n_redundant=2,\n",
                "                           n_classes=2, weights=[0.7], class_sep=0.7, random_state=15)\n",
                "\n",
                "x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.2, train_size=0.8)"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "source": [
                "def decision_function(Xcv):\n",
                "  sample_output_rbf = 0\n",
                "  y_predicted = []\n",
                "  gamma = clf._gamma\n",
                "  \n",
                "  for x_q in Xcv:\n",
                "    kernel_sum = 0\n",
                "    for i in range(len(support_vectors)):\n",
                "      squared_distance = (np.linalg.norm(support_vectors[i] - x_q)**2)\n",
                "      rbf_k = np.exp(-gamma * (squared_distance))\n",
                "      kernel_sum += dual_coeff[i]*rbf_k    \n",
                "    \n",
                "    sample_output_rbf = kernel_sum + intercept\n",
                "    y_predicted.append(sample_output_rbf[0])\n",
                "    \n",
                "  return np.array(y_predicted)    \n",
                "      "
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "source": [
                "# Randomly initializing weights (w) and intercept values (b)\n",
                "# dim — size of the w vector we want (or number of features or parameters in this case)\n",
                "def initialize_weights(dim):\n",
                "    ''' In this function, we will initialize our weights and bias\n",
                "    w — weights, a numpy array of size\n",
                "    b — bias, a scalar\n",
                "    '''\n",
                "    #initialize the weights to zeros array of (1,dim) dimensions\n",
                "    #you use zeros_like function to initialize zero, check this link https://docs.scipy.org/doc/numpy/reference/generated/numpy.zeros_like.html\n",
                "    #initialize bias to zero\n",
                "    w = np.zeros_like(dim)\n",
                "    # Above will initialize all w with 0.\n",
                "    b = 0\n",
                "    return w,b\n",
                "  \n",
                "def sigmoid(z):\n",
                "    return 1.0/(1 + np.exp(-z)) \n",
                "\n",
                "\n",
                "\n",
                "def logloss(y_true, y_pred):\n",
                "    \n",
                "    len_y_true = len(y_true)\n",
                "    \n",
                "    number_of_plus = np.count_nonzero(y_true == 1)\n",
                "    number_of_minus = np.count_nonzero(y_true == 0)\n",
                "    \n",
                "    # Plat scaling\n",
                "    # we will calculate y+, y- based on data points in train data\n",
                "    y_plus = (number_of_plus+1)/(number_of_minus+2)\n",
                "    y_minus = 1/(number_of_minus+2)\n",
                "    \n",
                "    sum_of_loss = 0\n",
                "    \n",
                "    for i in range(0, len_y_true):\n",
                "      if (y_true[i] == 1):\n",
                "        sum_of_loss += ((y_plus * np.log10(y_pred[i])) + ((1- y_plus) * np.log10(1-y_pred[i])))\n",
                "      else:\n",
                "        sum_of_loss += ((y_minus * np.log10(y_pred[i])) + ((1 - y_minus) * np.log10(1-y_pred[i])))\n",
                "        \n",
                "    loss = (-1/len_y_true) * sum_of_loss  \n",
                "    return loss\n",
                "    \n",
                "    \n",
                "  \n",
                "def gradient_dw(x,y,w,b,alpha,N):\n",
                "    '''In this function, we will compute the gardient w.r.to w '''\n",
                "    z = np.dot(w, x) + b\n",
                "    dw = x*(y - sigmoid(z)) - ((alpha)*(1/N) * w)\n",
                "    return dw\n",
                "  \n",
                "  \n",
                "def gradient_db(x,y,w,b):\n",
                "    z = np.dot(w, x) + b\n",
                "    db = y - sigmoid(z)\n",
                "\n",
                "    return db"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "source": [
                "def train(X_train, y_train, X_test, y_test, epochs, alpha, eta0, tol=1e-3):\n",
                "    \"\"\" In this function, we will implement logistic regression\"\"\"\n",
                "    # Here eta0 is learning rate\n",
                "    # implement the code as follows\n",
                "    # initialize the weights (call the initialize_weights(X_train[0]) function)\n",
                "    w, b = initialize_weights(X_train[0])\n",
                "    # for every epoch\n",
                "    train_loss = []\n",
                "    test_loss = []\n",
                "    N = len(X_train)\n",
                "\n",
                "    loss_threshold = 0.0001\n",
                "\n",
                "    for epoch in range(epochs):\n",
                "        # for every data point(X_train,y_train)\n",
                "        for row in range(N - 1):\n",
                "            # compute gradient w.r.to w (call the gradient_dw() function)\n",
                "            delta_weights = gradient_dw(\n",
                "                X_train[row], y_train[row], w, b, alpha, len(X_train)\n",
                "            )\n",
                "\n",
                "            # compute gradient w.r.to b (call the gradient_db() function)\n",
                "            delta_bias = gradient_db(X_train[row], y_train[row], w, b)\n",
                "\n",
                "            # update w, b\n",
                "            w = w + eta0 * delta_weights\n",
                "            b = b + eta0 * delta_bias\n",
                "\n",
                "        # predict the output of x_train[for all data points in X_train] using w,b\n",
                "        # y_prediction_train is a numpy array (vector) containing all predictions (0/1) for the examples in X\n",
                "        y_prediction_train = [  \n",
                "        sigmoid(np.dot(w, x_row) + b) for x_row in X_train\n",
                "        ]\n",
                "\n",
                "        # compute the loss between predicted and actual values (call the loss function)\n",
                "        # store all the train loss values in a list\n",
                "        train_loss.append(logloss(y_train, y_prediction_train))\n",
                "\n",
                "        # predict the output of x_test[for all data points in X_test] using w,b\n",
                "        y_prediction_test = [\n",
                "            sigmoid(np.dot(w, x_row) + b) for x_row in X_test\n",
                "        ]\n",
                "\n",
                "        print(\n",
                "            f\"For EPOCH No : {epoch} Train Loss is : {logloss(y_train, y_prediction_train)} and Test Loss is : {logloss(y_test, y_prediction_test)}\"\n",
                "        )\n",
                "\n",
                "        # compute the loss between predicted and actual values (call the loss function)\n",
                "        test_loss.append(logloss(y_test, y_prediction_test))\n",
                "\n",
                "      \n",
                "\n",
                "    return w, b, train_loss, test_loss"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "source": [
                "alpha=0.0001\n",
                "eta0=0.0001\n",
                "# N=len(X_train)\n",
                "epochs=50\n",
                "\n",
                "w, b, cv_log_loss, test_loss = train(x_train, y_train, x_test, y_test, epochs, alpha, eta0)\n",
                "print('w_coef ', w)\n",
                "print('intercept b ', b)"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "For EPOCH No : 0 Train Loss is : 0.2744419344800745 and Test Loss is : 0.27396804375413747\n",
                        "For EPOCH No : 1 Train Loss is : 0.25388270599770685 and Test Loss is : 0.2530470672889054\n",
                        "For EPOCH No : 2 Train Loss is : 0.2378933473945702 and Test Loss is : 0.23678528686907455\n",
                        "For EPOCH No : 3 Train Loss is : 0.22535914670131452 and Test Loss is : 0.22404811138173356\n",
                        "For EPOCH No : 4 Train Loss is : 0.2154510564869265 and Test Loss is : 0.2139901088303104\n",
                        "For EPOCH No : 5 Train Loss is : 0.2075576774367939 and Test Loss is : 0.2059869660056321\n",
                        "For EPOCH No : 6 Train Loss is : 0.20122721364761822 and Test Loss is : 0.1995769993734065\n",
                        "For EPOCH No : 7 Train Loss is : 0.1961232499884324 and Test Loss is : 0.19441634384670936\n",
                        "For EPOCH No : 8 Train Loss is : 0.19199265848161423 and Test Loss is : 0.19024629761068107\n",
                        "For EPOCH No : 9 Train Loss is : 0.18864275625360785 and Test Loss is : 0.1868700126719687\n",
                        "For EPOCH No : 10 Train Loss is : 0.18592512476594525 and Test Loss is : 0.18413595061839877\n",
                        "For EPOCH No : 11 Train Loss is : 0.18372411161637026 and Test Loss is : 0.1819261115810273\n",
                        "For EPOCH No : 12 Train Loss is : 0.1819485990950416 and Test Loss is : 0.18014760158224546\n",
                        "For EPOCH No : 13 Train Loss is : 0.18052605425104204 and Test Loss is : 0.17872653533946092\n",
                        "For EPOCH No : 14 Train Loss is : 0.1793981817222153 and Test Loss is : 0.17760358147778926\n",
                        "For EPOCH No : 15 Train Loss is : 0.1785177120798343 and Test Loss is : 0.17673067205968207\n",
                        "For EPOCH No : 16 Train Loss is : 0.1778460026110075 and Test Loss is : 0.1760685453916722\n",
                        "For EPOCH No : 17 Train Loss is : 0.17735122550615595 and Test Loss is : 0.17558489132073285\n",
                        "For EPOCH No : 18 Train Loss is : 0.17700698530272246 and Test Loss is : 0.17525293674068224\n",
                        "For EPOCH No : 19 Train Loss is : 0.17679125334954554 and Test Loss is : 0.1750503561125544\n",
                        "For EPOCH No : 20 Train Loss is : 0.17668553882952096 and Test Loss is : 0.1749584244106994\n",
                        "For EPOCH No : 21 Train Loss is : 0.1766742380660308 and Test Loss is : 0.17496135268769816\n",
                        "For EPOCH No : 22 Train Loss is : 0.176744119481736 and Test Loss is : 0.1750457625162668\n",
                        "For EPOCH No : 23 Train Loss is : 0.1768839127150246 and Test Loss is : 0.17520026700400704\n",
                        "For EPOCH No : 24 Train Loss is : 0.1770839784052766 and Test Loss is : 0.17541513429830508\n",
                        "For EPOCH No : 25 Train Loss is : 0.17733604096850783 and Test Loss is : 0.17568201546436332\n",
                        "For EPOCH No : 26 Train Loss is : 0.17763297094141742 and Test Loss is : 0.17599372298829039\n",
                        "For EPOCH No : 27 Train Loss is : 0.17796860661825117 and Test Loss is : 0.1763440493855824\n",
                        "For EPOCH No : 28 Train Loss is : 0.17833760705097962 and Test Loss is : 0.17672761780159452\n",
                        "For EPOCH No : 29 Train Loss is : 0.1787353302473546 and Test Loss is : 0.1771397582991189\n",
                        "For EPOCH No : 30 Train Loss is : 0.1791577317384783 and Test Loss is : 0.17757640489830942\n",
                        "For EPOCH No : 31 Train Loss is : 0.17960127970877984 and Test Loss is : 0.17803400948016282\n",
                        "For EPOCH No : 32 Train Loss is : 0.18006288366703746 and Test Loss is : 0.17850946946913274\n",
                        "For EPOCH No : 33 Train Loss is : 0.1805398342458837 and Test Loss is : 0.1790000668333707\n",
                        "For EPOCH No : 34 Train Loss is : 0.18102975219211018 and Test Loss is : 0.17950341642667067\n",
                        "For EPOCH No : 35 Train Loss is : 0.18153054498283194 and Test Loss is : 0.18001742207720206\n",
                        "For EPOCH No : 36 Train Loss is : 0.1820403697969484 and Test Loss is : 0.18054023912883696\n",
                        "For EPOCH No : 37 Train Loss is : 0.18255760180519734 and Test Loss is : 0.18107024237962213\n",
                        "For EPOCH No : 38 Train Loss is : 0.18308080692883688 and Test Loss is : 0.18160599855255646\n",
                        "For EPOCH No : 39 Train Loss is : 0.18360871836699472 and Test Loss is : 0.18214624258679252\n",
                        "For EPOCH No : 40 Train Loss is : 0.18414021631372238 and Test Loss is : 0.1826898571607575\n",
                        "For EPOCH No : 41 Train Loss is : 0.18467431038392706 and Test Loss is : 0.1832358549586729\n",
                        "For EPOCH No : 42 Train Loss is : 0.18521012434730919 and Test Loss is : 0.18378336327338765\n",
                        "For EPOCH No : 43 Train Loss is : 0.18574688283478905 and Test Loss is : 0.1843316106049731\n",
                        "For EPOCH No : 44 Train Loss is : 0.18628389973569792 and Test Loss is : 0.184879914969241\n",
                        "For EPOCH No : 45 Train Loss is : 0.18682056804827266 and Test Loss is : 0.18542767367538845\n",
                        "For EPOCH No : 46 Train Loss is : 0.1873563509827245 and Test Loss is : 0.18597435436929083\n",
                        "For EPOCH No : 47 Train Loss is : 0.18789077414663075 and Test Loss is : 0.1865194871699588\n",
                        "For EPOCH No : 48 Train Loss is : 0.1884234186678564 and Test Loss is : 0.1870626577524927\n",
                        "For EPOCH No : 49 Train Loss is : 0.18895391513146734 and Test Loss is : 0.187603501252488\n",
                        "w_coef  [ 0.04136468 -0.17919617 -0.73899583 -0.93469074  2.12216676]\n",
                        "intercept b  -0.6280337633128319\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 8,
            "source": [
                "x = np.array([i for i in range(0, 50)])\n",
                "\n",
                "train_log_loss_arr = np.array(cv_log_loss)\n",
                "\n",
                "plt.plot(x, train_log_loss_arr, \"-b\", label = 'Train log loss')\n",
                "\n",
                "plt.legend(loc=\"upper right\")\n",
                "plt.grid()\n",
                "\n",
                "plt.xlabel('Number of Epoch')\n",
                "plt.ylabel('Log Loss ')\n",
                "\n",
                "plt.title('Loss vs Epoch ')\n",
                "plt.show()"
            ],
            "outputs": [
                {
                    "output_type": "display_data",
                    "data": {
                        "text/plain": [
                            "<Figure size 432x288 with 1 Axes>"
                        ],
                        "image/png": "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"
                    },
                    "metadata": {
                        "needs_background": "light"
                    }
                }
            ],
            "metadata": {}
        }
    ],
    "metadata": {
        "orig_nbformat": 4,
        "language_info": {
            "name": "python"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}